{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca8ee4b2-3c7b-437f-94f0-8a7c18ff443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cfba5d4-4514-4c76-b55b-fcd728d7ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biaffine import BiaffineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f747a8ef-de89-4ce9-8bff-951967b93f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self, *initial_data, **kwargs):\n",
    "        for dictionary in initial_data:\n",
    "            for key in dictionary:\n",
    "                setattr(self, key, dictionary[key])\n",
    "        for key in kwargs:\n",
    "            setattr(self, key, kwargs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c3fa8c8-36be-4ad5-8a0b-8a52ad9ac6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(bert_model_path='/root/autodl-nas/pretrain-models/roberta-base',bert_feature_dim=768,biaffine_size=300,class_num=8,max_sequence_len=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cac71e7-3fa7-4b6d-8252-da59f4e34f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-nas/pretrain-models/roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BiaffineModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0606f84-b03f-4d88-89fb-54db33ab54d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4cdb37-f3c7-45f6-a594-53230af8c3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('savemodel/triplet_v11.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91620652-a66a-49c7-bfd2-d66b4b4ed6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4055ea30-88a6-48d4-ab0d-92a26f03675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3818e0c4-0d05-4499-a8fe-67225edb8f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,RobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73b4f0c-1aac-43fd-b5f8-88aa9305bfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.bert_model_path,add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc72027-cd48-4ad2-a669-0149eaffbf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(tokens):\n",
    "    tokens = ['##'] + tokens\n",
    "    sen_length = len(tokens)\n",
    "    token_range = []\n",
    "    bert_tokens = tokenizer.encode(tokens,is_split_into_words=True,truncation=True,max_length=args.max_sequence_len)\n",
    "    length = len(bert_tokens)\n",
    "    bert_tokens_padding = torch.zeros(args.max_sequence_len).long()\n",
    "    mask = torch.zeros(args.max_sequence_len)\n",
    "\n",
    "    for i in range(length):\n",
    "        bert_tokens_padding[i] = bert_tokens[i]\n",
    "    mask[:length] = 1\n",
    "\n",
    "    token_start = 1\n",
    "    for i, w, in enumerate(tokens):\n",
    "        token_end = token_start + len(tokenizer.encode(w, add_special_tokens=False))\n",
    "        token_range.append([token_start, token_end-1])\n",
    "        token_start = token_end\n",
    "    assert length == token_range[-1][-1]+2\n",
    "    return bert_tokens_padding,mask,token_range,sen_length,tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc263045-f02e-49af-8048-836c1f873900",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens_padding,mask,token_range,sen_length,tokens = convert('Again , competing products can generally intelligently split the wattage output between their available usb-c ports .'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eff04e1-f59d-4620-ae64-72b946e0c7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87f0050c-a40e-4b10-90e1-59d8144c459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(bert_tokens_padding.unsqueeze(0), mask.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f889be2f-379b-4f06-aba3-30292db7fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.argmax(preds, dim=3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90ef208e-1117-49c8-a676-424cc884f5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2637db66-75ae-4d56-a0cf-d5ac596abb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_BEGIN=1\n",
    "ASPECT_IN=2\n",
    "OPINION_BEGIN=3\n",
    "OPINION_IN=4\n",
    "PAIR=5\n",
    "sentiment2id = {'观点-负面': 5, '观点-中性':6, '观点-正面': 7}\n",
    "\n",
    "ASPECT=[ASPECT_BEGIN,ASPECT_IN]\n",
    "OPINION=[OPINION_BEGIN,OPINION_IN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "983e74dc-46a7-4afd-b55d-e3aac5e1642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2sentiment = {v:k for k,v in sentiment2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "907c593d-e4f0-4313-8de1-c267591bd89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(tags, length, token_range, type):\n",
    "    spans = []\n",
    "    start = -1\n",
    "    begin,mid = type\n",
    "    for i in range(length):\n",
    "        l, r = token_range[i]\n",
    "        if tags[l][l] == -1:\n",
    "            continue\n",
    "        elif tags[l][l] == begin:\n",
    "            if start != -1:\n",
    "                spans.append([start, i - 1])\n",
    "            start = i\n",
    "        elif tags[l][l] not in type:\n",
    "            if start != -1:\n",
    "                spans.append([start, i - 1])\n",
    "                start = -1\n",
    "    if start != -1:\n",
    "        spans.append([start, length - 1])\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab51c110-6bf1-4bb6-84b0-2f476d18ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_triplet(tags, aspect_spans, opinion_spans, token_ranges):\n",
    "    triplets = []\n",
    "    for al, ar in aspect_spans:\n",
    "        for pl, pr in opinion_spans:\n",
    "            tag_num = [0] * 8\n",
    "            for i in range(al, ar + 1):\n",
    "                for j in range(pl, pr + 1):\n",
    "                    a_start = token_ranges[i][0]\n",
    "                    o_start = token_ranges[j][0]\n",
    "                    if al < pl:\n",
    "                        tag_num[int(tags[a_start][o_start])] += 1\n",
    "                    else:\n",
    "                        tag_num[int(tags[o_start][a_start])] += 1\n",
    "\n",
    "            if sum(tag_num[5:]) == 0: continue\n",
    "            sentiment = -1\n",
    "            if tag_num[5] >= tag_num[6] and tag_num[5] >= tag_num[7]:\n",
    "                sentiment = 5\n",
    "            elif tag_num[6] >= tag_num[5] and tag_num[6] >= tag_num[7]:\n",
    "                sentiment = 6\n",
    "            elif tag_num[7] >= tag_num[5] and tag_num[7] >= tag_num[6]:\n",
    "                sentiment = 7\n",
    "            if sentiment == -1:\n",
    "                continue\n",
    "            triplets.append([al, ar, pl, pr, sentiment])\n",
    "         \n",
    "    ops = set([(x[2],x[3]) for x in triplets if x[0]!=0])\n",
    "    triplets = [x for x in triplets if not (x[0]==0 and (x[2],x[3]) in ops)]\n",
    "        \n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "205f6da1-d878-4798-a16a-e5eb2b27f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_aspect_spans = get_spans(preds, sen_length, token_range, ASPECT)\n",
    "predicted_opinion_spans = get_spans(preds, sen_length, token_range, OPINION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3201bc4-f4b4-4fe6-8173-a16f4d9a13a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = find_triplet(preds, predicted_aspect_spans,predicted_opinion_spans,token_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e6c9be5-51bc-4a56-b8c0-fac5212eb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67f305b7-2062-4884-9690-6d0b6c29bcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 3, 16, 5]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7923cb9-b28f-45b9-91b5-7aff5095472b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('##', 'competing products can generally intelligently split the wattage output between their available usb-c ports', '观点-负面')\n",
      "['competing', 'products', 'can', 'generally', 'intelligently', 'split', 'the', 'wattage', 'output', 'between', 'their', 'available', 'usb-c', 'ports'] ['Again', ',', '$ competing', 'products', 'can', 'generally', 'intelligently', 'split', 'the', 'wattage', 'output', 'between', 'their', 'available', 'usb-c', 'ports $', '.']\n"
     ]
    }
   ],
   "source": [
    "for al,ar,pl,pr,sentiment in triplets:\n",
    "    aspect = ' '.join(tokens[al:ar+1])\n",
    "    opinion = ' '.join(tokens[pl:pr+1])\n",
    "    sentiment = id2sentiment[sentiment]\n",
    "    \n",
    "    \n",
    "    print((aspect,opinion,sentiment))\n",
    "    \n",
    "    tt = deepcopy(tokens)\n",
    "    tt[al] = '# ' + tt[al]\n",
    "    tt[ar] = tt[ar] + ' #'\n",
    "    tt[pl] = '$ ' + tt[pl]\n",
    "    tt[pr] = tt[pr] + ' $'\n",
    "    s1 = tokens[al:ar+1] + tokens[pl:pr+1] if al > 0 else tokens[pl:pr+1]\n",
    "    s2 = tt[1:]\n",
    "    print(s1,s2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1f54b57-5741-4652-bd32-fdc161221b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = deepcopy(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9df3c7cb-d3e2-4565-94ba-a3d4e1fc73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ca_label_list = json.load(open('/root/autodl-nas/ABSA/通用/general-category-labels.json'))\n",
    "ca_label2id = {lb:i for i,lb in enumerate(ca_label_list)}\n",
    "ca_id2label = {v:k for k,v in ca_label2id.items()}\n",
    "num_labels = len(ca_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "540facfa-c90e-4627-93be-a0b0954f7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9c95f2a-9e06-4bbb-afe2-848682767a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_model():\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"/root/autodl-nas/pretrain-models/reviews-roberta/\", num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(\"/root/autodl-nas/ABSA/通用/model-p69-f54.bin\"))\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65dce5af-d9cd-4a82-8585-c7e47b588065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-nas/pretrain-models/reviews-roberta/ were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /root/autodl-nas/pretrain-models/reviews-roberta/ and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "category_model = get_category_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3bf35a6a-d52e-4db8-be63-6463e3234b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_predict(s1,s2):\n",
    "    tokenized_inputs = tokenizer(s1,s2,truncation=True,max_length=100,padding='max_length',is_split_into_words=True,return_tensors='pt')\n",
    "    tokenized_inputs = {k:v.to(device) for k,v in tokenized_inputs.items()}\n",
    "\n",
    "    outs = category_model(**tokenized_inputs)[0]\n",
    "    b_logit_pred = outs\n",
    "    b_probs = torch.softmax(b_logit_pred,dim=-1)\n",
    "    pred_label = torch.argmax(b_logit_pred,dim=-1)\n",
    "\n",
    "    b_probs = b_probs.detach().cpu().numpy()\n",
    "    pred_label = pred_label.to('cpu').numpy()\n",
    "    return [[ca_id2label[int(pl)],b_probs[i][int(pl)]] for i,pl in enumerate(pred_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0db77ca5-1f8b-4664-8e54-c33d856e23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beb63b41-b305-4f42-b09c-1ca40d16ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    text = text.lower()\n",
    "    stokens = nltk.word_tokenize(text)[:110]\n",
    "    try:\n",
    "        bert_tokens_padding,mask,token_range,sen_length,tokens = convert(stokens)\n",
    "    except:\n",
    "        return []\n",
    "    preds = model(bert_tokens_padding.unsqueeze(0).to(device), mask.unsqueeze(0).to(device))\n",
    "    preds = torch.argmax(preds, dim=3)[0]\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    predicted_aspect_spans = get_spans(preds, sen_length, token_range, ASPECT)\n",
    "    predicted_opinion_spans = get_spans(preds, sen_length, token_range, OPINION)\n",
    "    triplets = find_triplet(preds, predicted_aspect_spans,predicted_opinion_spans,token_range)\n",
    "    finals = []\n",
    "    for al,ar,pl,pr,sentiment in triplets:\n",
    "        aspect = ' '.join(tokens[al:ar+1])\n",
    "        opinion = ' '.join(tokens[pl:pr+1])\n",
    "        sentiment = id2sentiment[sentiment]\n",
    "        tt = deepcopy(tokens)\n",
    "        tt[al] = '# ' + tt[al]\n",
    "        tt[ar] = tt[ar] + ' #'\n",
    "        tt[pl] = '$ ' + tt[pl]\n",
    "        tt[pr] = tt[pr] + ' $'\n",
    "        s1 = tokens[al:ar+1] + tokens[pl:pr+1] if al > 0 else tokens[pl:pr+1]\n",
    "        s2 = tt[1:]\n",
    "        s2 = ' '.join(s2).split()\n",
    "        # lb,prob = category_predict(s1,s2)[0]\n",
    "        finals.append((aspect,opinion,sentiment))\n",
    "    return finals\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cd4d348-3c09-4a88-88d7-429a039e908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_judge(text):\n",
    "    stext = text.lower()\n",
    "    stokens = nltk.word_tokenize(stext)[:110]\n",
    "    \n",
    "    try:\n",
    "        bert_tokens_padding,mask,token_range,sen_length,tokens = convert(stokens)\n",
    "    except:\n",
    "        return None\n",
    "    tokens = ['##'] + nltk.word_tokenize(text)[:110]\n",
    "    preds = model(bert_tokens_padding.unsqueeze(0).to(device), mask.unsqueeze(0).to(device))\n",
    "    preds = torch.argmax(preds, dim=3)[0]\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    predicted_aspect_spans = get_spans(preds, sen_length, token_range, ASPECT)\n",
    "    predicted_opinion_spans = get_spans(preds, sen_length, token_range, OPINION)\n",
    "    triplets = find_triplet(preds, predicted_aspect_spans,predicted_opinion_spans,token_range)\n",
    "    finals = []\n",
    "    tags = ['O'] * len(tokens)\n",
    "    for al,ar,pl,pr,sentiment in triplets:\n",
    "        tags[al:ar+1] = ['I-评价维度'] * (ar-al+1)\n",
    "        tags[al] = 'B-评价维度'\n",
    "        tags[pl:pr+1] = ['I-'+id2sentiment[sentiment]] * (pr-pl+1)\n",
    "        tags[pl] = 'B-'+id2sentiment[sentiment]\n",
    "    return tags,tokens\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "caeb3b96-ddaf-4264-a449-e8f29bb91344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['O', 'B-观点-正面', 'B-评价维度', 'O', 'O', 'B-观点-正面', 'I-观点-正面'],\n",
       " ['##', 'nice', 'watch', 'at', 'a', 'great', 'price'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_for_judge('nice watch at a great price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4f09e64b-17fd-48f7-b3b1-a127df0d9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4be49efd-ff0a-4a72-8373-eff671489781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/root/autodl-nas/ABSA/raw_reviews/pen.csv').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e84a3cac-600e-4b01-89e5-1a70d7a6a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b78bcc87-8673-489e-81d5-e70302c0fe2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star\n",
       "5    3822\n",
       "4     377\n",
       "1     329\n",
       "3     278\n",
       "2     194\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.value_counts('star')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7d6e960-a64b-4802-a5da-eec6e0bf8b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sample.to_dict('records')\n",
    "five = [x for x in sample if x['star'] in [4,5]]\n",
    "others = [x for x in sample if x['star'] not in [4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a580422-5802-492b-a857-a8cd661cd0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c54f81d-71b8-4f3a-af81-3afcec466990",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(five)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "573d06bf-d61b-46dd-a33b-8ec2fb1b8824",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = others + five[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43f857c2-0e4a-424f-9a01-e75f1550030c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1301"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9656f45d-979e-4d25-8e29-e0e3d20ff327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1c74b09-38e4-48bb-9424-4288d1f4894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "594f775b-21e2-4832-992f-89c95e5ae739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = [json.loads(x) for x in open('/root/autodl-tmp/xueyou/ABSA/absa待打标数据/juying_sample_1693.jsonl')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0283e66a-5b67-4c2c-9fca-c10b8c2559c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ff7ee1d-5341-4606-85fd-3b20b603300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c86fd8f2-bce7-493a-8f75-2569feb510da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1301/1301 [00:24<00:00, 53.38it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "for x in tqdm(sample):\n",
    "    content = x['content']\n",
    "    for p in content.split('\\n'):\n",
    "        for s in nltk.sent_tokenize(p):\n",
    "            if len(s) < 5:\n",
    "                continue\n",
    "            try:\n",
    "                if langdetect.detect(s) !='en':\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "            texts.append({'id':x['reviewId'],'star':x['star'],'text':s})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b61d541-5ed1-4059-b4a5-63764cec4516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3665"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c687ad43-2cce-45f9-aca2-6bda6d0715d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'R6VHNJZI2YI5F',\n",
       " 'star': 2,\n",
       " 'text': 'Pens only 2/3 to 3/4 filled with ink.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[556]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3dbc7083-61e2-4c82-9636-ad9bdea93ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pens', 'only 2/3 to 3/4 filled with ink', '观点-负面')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(texts[556]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d4796f7-cab1-4021-bb98-657a1581c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:34<00:00, 88.13it/s] \n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for x in tqdm(texts[:3000]):\n",
    "    ret = predict_for_judge(x['text'])\n",
    "    if ret:\n",
    "        data.append((x,ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0c35e87-a2b9-4a63-8283-82298afca369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_span(words,tags):\n",
    "    word = []\n",
    "    tag = ''\n",
    "    tag_words = []\n",
    "    for i,(c,t) in enumerate(zip(words,tags)):\n",
    "        if t[0] in ['B','S','O']:\n",
    "            if word:\n",
    "                tag_words.append((word,i,tag))\n",
    "            if t[0] == 'O':\n",
    "                word = []\n",
    "                tag = ''\n",
    "                continue\n",
    "            word = [c]\n",
    "            tag = t[2:]\n",
    "        else:\n",
    "            word.append(c)\n",
    "    if word:\n",
    "        tag_words.append((word,i+1,tag))\n",
    "\n",
    "    return [(b-len(a),b,c) for a,b,c in tag_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a9fe1dd-5175-454f-ba71-483e759b2bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3e0f824-c038-48c9-b272-003889b70365",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/root/autodl-tmp/xueyou/ABSA/absa待打标数据/pen_judge_ABSA.jsonl','w') as f:\n",
    "    for doc,(tag,tokens) in data:\n",
    "        tokens = tokens[1:]\n",
    "        tag = tag[1:]\n",
    "        spans = get_span(tokens,tag)\n",
    "        label = []\n",
    "        for s,e,l in spans:\n",
    "            s = len(' '.join(tokens[:s])) + 1 if s > 0 else 0\n",
    "            e = len(' '.join(tokens[:e]))\n",
    "            label.append([s,e,l])\n",
    "        f.write(json.dumps({'text':' '.join(tokens),'label':label,'star':doc['star'],'reviewId':doc['id']},ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "119c93fb-4863-447b-a472-f81f43bdf75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 10, '评价维度'], [24, 29, '观点-正面'], [126, 130, '评价维度'], [131, 155, '观点-负面']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc4d099-d229-4d33-93d1-82c21f7b13a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1278c77-5dd9-41a7-a0d0-30431f080728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_predict(texts):\n",
    "    batch = []\n",
    "    for item in texts:\n",
    "        text = item['text']\n",
    "        text = text.lower()\n",
    "        stokens = nltk.word_tokenize(text)[:110]\n",
    "        try:\n",
    "            bert_tokens_padding,mask,token_range,sen_length,tokens = convert(stokens)\n",
    "        except:\n",
    "            continue\n",
    "        batch.append((item,bert_tokens_padding,mask,token_range,sen_length,tokens))\n",
    "    preds = model(torch.stack([x[1] for x in batch]).to(device), torch.stack([x[2] for x in batch]).to(device))\n",
    "    preds = torch.argmax(preds, dim=3)\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "    finals = []\n",
    "    for i in range(len(batch)):\n",
    "        predicted_aspect_spans = get_spans(preds[i], batch[i][-2], batch[i][-3], ASPECT)\n",
    "        predicted_opinion_spans = get_spans(preds[i], batch[i][-2], batch[i][-3], OPINION)\n",
    "        triplets = find_triplet(preds[i], predicted_aspect_spans,predicted_opinion_spans,batch[i][-3])\n",
    "        tokens = batch[i][-1]\n",
    "        finals.append((batch[i][0],tokens,triplets))\n",
    "        # s1s,s2s = [],[]\n",
    "        # aspects,opinions,sentiments = [],[],[]\n",
    "        # for al,ar,pl,pr,sentiment in triplets:\n",
    "        #     aspect = ' '.join(tokens[al:ar+1])\n",
    "        #     opinion = ' '.join(tokens[pl:pr+1])\n",
    "        #     sentiment = id2sentiment[sentiment]\n",
    "        #     aspects.append(aspect)\n",
    "        #     opinions.append(opinion)\n",
    "        #     sentiments.append(sentiment)\n",
    "        #     tt = deepcopy(tokens)\n",
    "        #     tt[al] = '# ' + tt[al]\n",
    "        #     tt[ar] = tt[ar] + ' #'\n",
    "        #     tt[pl] = '$ ' + tt[pl]\n",
    "        #     tt[pr] = tt[pr] + ' $'\n",
    "        #     s1 = tokens[al:ar+1] + tokens[pl:pr+1] if al > 0 else tokens[pl:pr+1]\n",
    "        #     s2 = tt[1:]\n",
    "        #     s2 = ' '.join(s2).split()\n",
    "        #     s1s.append(s1)\n",
    "        #     s2s.append(s2)\n",
    "        # categorys = category_predict(s1s,s2s)\n",
    "        # for i,((lb,prob),aspect,opinion,sentiment) in enumerate(zip(categorys,aspects,opinions,sentiments)):\n",
    "        #     finals.append((batch[i][0]['id'],aspect,opinion,sentiment,lb,prob))\n",
    "    return finals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65aa2d71-ce48-4d60-8b9d-1124432494f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'text': 'I love the watch.', 'id': 1},\n",
       "  ['##', 'i', 'love', 'the', 'watch', '.'],\n",
       "  [[4, 4, 2, 2, 7]])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_predict([{'text':'I love the watch.','id':1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dbc3822a-5ce3-4566-bdd6-57d7afd6d001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a74f329-d43c-4c4f-8d8f-a2c3a7553b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "54f6e938-3ef0-49be-8567-e58b8ad88f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "77254602-9a22-4036-a686-c2d3566c1110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "996a71f6-bdc7-49d5-9ba1-3109521eee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in glob('/root/autodl-nas/ABSA/Anker_reviews/*.csv'):\n",
    "    if 'acdc.csv' in fname or 'absa' in fname:\n",
    "        continue\n",
    "    print('process',fname)\n",
    "    finals = []\n",
    "    df = pd.read_csv(fname).fillna('')\n",
    "    print('raw reviews',len(df))\n",
    "    texts = []\n",
    "    for x in tqdm(df.to_dict('records')):\n",
    "        content = x['content']\n",
    "        for p in content.split('\\n'):\n",
    "            for s in nltk.sent_tokenize(p):\n",
    "                if len(s) < 5:\n",
    "                    continue\n",
    "                texts.append({'id':x['reviewId'],'star':x['star'],'text':s})\n",
    "    print('raw texts',len(texts))\n",
    "    for i in tqdm(range(0,len(texts),batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        outs = batch_predict(batch)\n",
    "        finals.extend(outs)\n",
    "\n",
    "    s1s,s2s = [],[]\n",
    "    idxs = []\n",
    "    aspects,opinions,sentiments = [],[],[]\n",
    "    for item,tokens,triplets in finals:\n",
    "        for al,ar,pl,pr,sentiment in triplets:\n",
    "            aspect = ' '.join(tokens[al:ar+1])\n",
    "            opinion = ' '.join(tokens[pl:pr+1])\n",
    "            sentiment = id2sentiment[sentiment]\n",
    "            aspects.append(aspect)\n",
    "            opinions.append(opinion)\n",
    "            sentiments.append(sentiment)\n",
    "            tt = deepcopy(tokens)\n",
    "            tt[al] = '# ' + tt[al]\n",
    "            tt[ar] = tt[ar] + ' #'\n",
    "            tt[pl] = '$ ' + tt[pl]\n",
    "            tt[pr] = tt[pr] + ' $'\n",
    "            s1 = tokens[al:ar+1] + tokens[pl:pr+1] if al > 0 else tokens[pl:pr+1]\n",
    "            s2 = tt[1:]\n",
    "            s2 = ' '.join(s2).split()\n",
    "            s1s.append(s1)\n",
    "            s2s.append(s2)\n",
    "            idxs.append(item)\n",
    "    outs = []\n",
    "    for i in tqdm(range(0,len(idxs),batch_size)):\n",
    "        categorys = category_predict(s1s[i:i+batch_size],s2s[i:i+batch_size])\n",
    "        for j,((lb,prob),aspect,opinion,sentiment) in enumerate(zip(categorys,aspects[i:i+batch_size],opinions[i:i+batch_size],sentiments[i:i+batch_size])):\n",
    "            outs.append((idxs[i+j],aspect,opinion,sentiment,lb,prob))\n",
    "\n",
    "    df = pd.DataFrame([{'id':x[0]['id'],'aspect':x[1],'opinion':x[2],'sentiment':x[3],'category':x[4]} for x in outs if x[-1]>=0.2])\n",
    "    df.to_csv(fname.replace('.csv','_absa.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9ceab2a-88ed-4a10-b4f7-7b3dd5b8b3b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_592755/446623318.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_absa.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fname' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(fname.replace('.csv','_absa.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bf2d4-0324-4911-bac7-8dc13c09c81e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bb243-a797-4c50-af80-30ecf21a04f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75410831-8dad-4abf-a807-80dbf8d7d48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
